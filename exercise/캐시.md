# 빠르고 확장성 있는 데이터 액세스를 위한 빌딩 블록
참고 : https://d2.naver.com/helloworld/206816

수 테라바이트 크기의 데이터를 메모리에 올리는 것은 매우 높은 비용이 필요하기 때문에,<br>
모든 데이터를 메모리에 저장하지 않으면서도 빠른 액세스가 가능하도록 하는 것은 매우 어려운 도전 과제가 된다.<br>

여기서 성능에 가장 영향을 미치는 것은 디스크 I/O다. <br>
디스크에서 데이터를 읽는 것은 메모리에서 데이터를 읽는 것보다 훨씬 더 느리다. <br>
데이터가 많을 때에는 이러한 속도 차이가 더 크게 나타난다. <br>

실제로 메모리 액세스는 디스크 순차적 읽기에 비하여 최소 6배 빠르고, 디스크 랜덤 읽기에 대해서는 십만 배 더 빠르다.<br>
게다가 유일 식별자(unique ID)처럼 크기가 작은 데이터를 찾는 것 또한 매우 어려운 일이 될 수도 있다. <br>
이는 흡사 방안의 어딘가에 있는 사탕을 눈을 감고 찾는 것과 같다.

고맙게도 이런 것들을 쉽게 하기 위한 다양한 방법이 있다. <br>
그 중 가장 중요한 네 가지 방법에는 
- 캐시
- 프락시
- 인덱스
- 로드 밸런서

가 있다. 

각각의 방법이 어떻게 데이터 액세스를 빠르게 하는지에 대해서 알아보자.
(여기서는 캐시만.)

## 캐시
캐시는 최근에 요청받은 데이터는 다시 요청받을 확률이 높다는 지역성의 원리(locality of reference)에 기반한 방법이다.
캐시는 
- 하드웨어 
- 운영체제
- 웹 브라우저
- 웹 애플리케이션 

등 다양한 곳에서 사용하고 있다. 

캐시란 매우 짧은 시간 동안 유지되는 메모리와 같은 것이다. <br>
캐시 용량은 매우 제한적이지만, 통상적으로 원래의 데이터 저장소보다는 매우 빠르고 자주 액세스되는 데이터를 보유하고 있다. 

캐시는 아키텍처의 모든 단계에 위치할 수 있지만, 프런트엔드와 가까운 곳에 위치하는 경우가 많다. <br>
<b style="color: red">왜냐하면 보통 캐시는 서비스의 백엔드까지 가는 시간적인 비용을 줄이기 위해서 사용하는 경우가 많기 때문이다.</b>

이미지 서버 아키텍처에서 빠른 데이터 액세스를 가능하게 하기 위하여 캐시를 어디에서 사용할 수 있을까? <br>
이 경우 캐시를 추가할 수 있는 두 가지 선택이 있다. 하나는 아래와 같이 캐시를 요청 노드에 추가하는 방법이다.

![요청노드](https://user-images.githubusercontent.com/19552819/100873035-1129b300-34e6-11eb-9d9a-639d31267642.JPG)

캐시를 요청 노드에 배치하는 것은 응답 데이터를 로컬 저장 공간에서 가져올 수 있게 한다.<br>
매번 요청은 서비스로 보내지고, 요청 노드에 데이터가 존재하면 그 노드는 재빠르게 로컬에서 캐싱된 데이터를 보낸다.<br>
(없다면, 디스크에서 데이터를 질의)

요청 노드를 여러 개로 확장시키면 각 노드가 각각의 캐시를 가질 수 있게 된다.<br>
만약 로드 밸런서가 임의로 요청을 분산시키면, 같은 요청이 다른 노드로 가게 될 수도 있다. 

즉, 캐시 미스가 증가하게 될 것이다. <br>
캐시 미스를 줄이면서 여러 개의 캐시를 사용하기 위해 사용하는 방법이 전역 캐시와 분산 캐시다.

## 전역 캐시(Global Cache)
전역 캐시는 말 그대로 모든 노드가 오직 하나의 캐시 공간만을 사용한다.<br>
전역 캐시는 서버나 어떤 종류의 파일 저장소를 추가해도 잘 동작하고, 원래의 저장소보다 빠르며 모든 요청 레이어 노드에서 접근이 가능하다.

요청 노드에서 각각의 요청은 로컬에 캐시를 가지고 있는 것과 마찬가지 방법으로 글로벌 캐시에 데이터를 질의한다.<br>

이러한 종류의 캐시 사용은 클라이언트의 개수나 요청 개수가 급격하게 증가하면<br>
하나의 캐시가 그 요청을 감당하지 못할 수도 있기 때문에 복잡해지기 쉽다.<br>

하지만 이러한 아키텍처가 특정한 상황에서는 매우 유용하다.<br>
ex) 특화된 하드웨어를 써서 전역캐시를 빠르게 만들거나, 캐시가 필요한 데이터의 양이 고정된 일정량일 때

전역 캐시에는 두 가지의 일반적인 방식이 있다.<br>

1. 데이터 노드는 오직 캐시에만 데이터를 질의하고 전역 캐시는 요청받은 데이터를 자기 자신에서 찾을 수 없을 때,<br>
캐시 스스로가 저장 공간에 데이터를 질의하여 요청 노드에 데이터를 전달하도록 하는 방식이다.

![전역캐시1](https://user-images.githubusercontent.com/19552819/100873036-1129b300-34e6-11eb-9379-b69e794229a4.JPG)

2. 요청 노드가 전역 캐시에서 데이터를 질의하여 데이터가 없음을 확인하였을 때, 직접 스토리지에 질의하여 데이터를 가져오는 방식이다.

![전역캐시2](https://user-images.githubusercontent.com/19552819/100873037-1129b300-34e6-11eb-8912-c1a2b1018ebd.JPG)

전역캐시를 사용하는 대부분의 애플리케이션은 같은 요청이 여러 요청 노드로부터 발생하는 것을 막기 위해<br>
캐시 스스로가 데이터 축출과 조회를 직접하는 (1)과 같은 아키텍처를 사용한다.

그러나 (2)와 같은 경우가 더 유용할 때도 있다.
- 큰 크기의 파일 제공을 위하여 캐시를 사용하는 경우
  - 낮은 캐시 히트가 발생하면 전반적인 캐시 미스가 증가하게 된다.
  - 이 경우에는 자주 사용되는 데이터만 캐시에 위치하게 하는 것이 도움이 된다.
- 정적 파일을 캐시에 저장하는 경우
  - 이 경우에는 정적 파일은 절대 캐시에서 지워지지 않는다.
    - 레이턴시에 대한 애플리케이션 요구사항 때문이다. 
    - 많은 데이터 처리를 위하여 데이터 일부는 빠르게 전송될 필요가 있을 수 있다. 
    - 이런 경우에는 캐시 자체보다는 애플리케이션 로직이 캐시 축출이나 핫 스팟 관리를 하는 것이 좋다.

## 분산 캐시(Distributed Cache)
분산 캐시는 아래와 같이 각각의 노드가 캐시 데이터를 갖는 방식이다.<br>

![분산캐시1](https://user-images.githubusercontent.com/19552819/100873032-0ff88600-34e6-11eb-925d-0d9263688a47.JPG)

일반적으로 분산 캐시는 consistent hashing 함수를 사용한다. 따라서 요청 노드가 어떤 특정한 데이터 조각을 찾으려 할 때<br>
해시 함수를 이용해 분산 캐시 내의 어디에서 데이터를 찾을 수 있는지 알 수 있다.

각각의 노드는 각각의 조그마한 캐시를 가지고 있다.<br>
그리고 요청이 들어오면 원본 저장 공간으로 요청을 보내기 전에 다른 노드에 요청을 보낸다.<br>
분산 캐시의 이런 점 때문에 요청 풀에 노드를 추가하면 전체 캐시 크기를 증가시킬 수 있다.<br>
(즉, 요청 -> 로컬 캐시 확인 -> 다른 노드의 로컬 캐시 확인 -> 디스크 순서인 건가?)

분산 캐시의 단점은 장애가 발생한 노드를 처리하는 방법이 필요하다는 것이다.<br>
다른 노드에 여러 개의 복제본을 가지는 방법으로 해결하기도 한다. 

이런 방식을 사용하면 문제 노드를 처리하기 위한 로직이 복잡해지기 십상이다. <br>
요청 레이어에 새로운 노드를 추가하거나 제거하려 할 때 특히 그렇다. <br>

물론 노드가 사라지고 캐시의 일부분이 분실되더라도 원본 데이터에 요청함으로써 필요한 데이터를 가져올 수 있다.<br>
그렇기 때문에 분산 캐시에 장애가 발생한다고 해서 총체적인 장애가 발생하는 것은 아니다.(OpenStack Swift랑 비슷한 것 같다.)

오픈소스 캐시 중 인기 있는 것 중 하나는[Memcached](http://memcached.org/)다(로컬캐시나 분산캐시 두 가지 모드로 동작 가능하다)

Facebook은 또한 많은 서버에 분산되어 있는 전역 캐시를 사용한다([참고](https://www.facebook.com/note.php?note_id=39391378919)).<br>
단 한 번의 함수 콜로 서로 다른 Memcached 서버에 접근하는 병렬 요청을 만들어 낼 수 있다

## 프락시와 캐시
프락시와 캐시를 함께 사용하는 것은 무의미하지만, 같이 사용하게 될 때에는 캐시를 프락시 앞에 두는 것이 최선이다. <br>
많은 사람들이 참여하는 마라톤 레이스에서 빠른 주자들이 먼저 출발하는 것처럼 말이다. <br>
캐시는 데이터를 메모리에서 가져오고 보통은 매우 빠르다. 

그리고 같은 결과를 반환하는 여러 개의 요청도 문제가 되지 않는다. <br>
이 때문에 프락시가 캐시 앞에 있으면 캐시로 요청이 오기 전에 추가적인 지연만 생기고 성능이 저하된다.

## HTTP와 캐시
HTTP는 state-less하다. 즉, 주고받은 문서는 상태를 갖지 않으므로, 캐시하기 쉽다.<br>
따라서, HTTP에는 프로토콜 레벨에서 캐시 기능이 내장되어 있다.

캐시 서버는 결국, 클라이언트와의 통신 뿐만 아니라, 서버간 HTTP간의 통신이라면 사용을 고려해보면 좋다.<br>
LAN 게이트웨이 앞에 캐시서버를 두고, 캐시서버를 경유해서 리얼서버로 접속하도록 하는 프록시 서버와 같은 구조로 두면 좋다.

서버의 부하분산이라는 관점에서 보면 캐시서버를 프록시로 이용하는 것이 주된 이용 형태이다.<br>
클라이언트로부터 요청된 페이지가 캐싱되어있다면 굳이 APP으로 전달할 필요없이 캐시서버에 캐시 되어있는 것을 반환하면,<br>
원래 사이트에 여러번 접속할 필요가 없고 대역폭이 절약된다.

대표적으로 squid(HTTP/FTP등에서 이용되는 오픈소스 캐시서버) 등이 있다.

## memcached에 의한 캐시
- HTTP 레벨의 캐시로는 적절하지 않은 경우도 많다.
  - APP 내부에서 이용하는 데이터의 단위 크기로 캐시를 관리하는 캐시 서버를 사용할 수도 있다.
  - memcached가 그 일례이다.
- memcached는 C언어로 작성된 고속 네트워크에 알맞은 분산 캐시서버이다.
  - 스토리지로는 OS의 메모리를 이용한다.
  - 클라이언트 라이브러리를 이용하여 서버와 통신하면서
  - 프로그래밍 언어가 규정하는 객체를 읽어오고 저장할 수 있다.
  - memcached는 프로그램 내부에서 이용한다.

